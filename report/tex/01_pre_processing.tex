\section{Pre-processing data}
To complete the pre-processing of the original dataset, I used Pandas and Numpy. The first step for pre-processing the original file, after downloading it, was to remove all rows that had issues. Specifically speaking, this issue is not having either person name or ID. If either of these two information are missing, the row is removed from the original set. After removing those rows, I also removed those columns that did not have a label i.e., a question, and the two last questions that were further apart from the first 7. Furthermore, in order to have a more intelligible data set, I renamed the first column from \textit{"Unnamed: 0"} to \textit{"Name"}. Then, I isolated all questions and for each one of them, I created a new data set comprised of three attributes: \textit{"Name"}, \textit{"ID"}, \textit{"Question"}. This resulted in 7 new data sets, one for each question, which were saved as CSV files. The questions are:

\begin{enumerate}
    \item "Which person you have hear of their voice or seen their faces?"
    \item "Which person you have met (in person+online) and exchange conversation?"
    \item "Which person you have collaborated with?"
    \item "Which person you have eye contact?"
    \item "Which peson you have eaten lunch with?"
    \item "Which person you have shared a ride?"
    \item "Which person you have taken at least two courses with?"
\end{enumerate}

After having separated the questions, I processed the inputs of each question in each individual file. The reason for this was to simply adequate each person's input to comma-separated IDs by removing spaces and trailing characters such as commas and hyphens. Finally, for each of the 7 questions, I created 7 other files in the CSV format Gephi expects. For example, a line with the following values \textit{"1,2,3,4,5"} means that node 1 is connected to nodes 2, 3, 4 and 5.